"""
Analyze ARC-AGI tasks to identify which curriculum skills they require.

This script examines each puzzle and attempts to identify which of our
13 core cognitive skills are needed to solve it.
"""

import json
import os
from pathlib import Path
from collections import defaultdict, Counter
import numpy as np

# Skill categories from our curriculum
SKILLS = {
    # Core Priors
    "object_cognition": "Object detection, boundaries, tracking",
    "numerosity": "Counting, quantity comparison, arithmetic",
    "geometry": "Shapes, symmetry, rotation, scale",
    "topology": "Containment, connectivity, proximity",
    "physics": "Gravity, support, contact, occlusion",
    
    # Cognitive Operations
    "pattern_recognition": "Repetition, periodicity, hierarchy",
    "transformation": "Translation, rotation, color mapping",
    "analogy": "Structural analogy, correspondence",
    "goal_reasoning": "Goal specification, planning, constraints",
    "hypothesis_testing": "Rule induction, verification, revision",
    
    # Meta-Cognitive
    "attention": "Selective focus, feature binding",
    "working_memory": "Chunking, integration, maintenance",
    "search": "Systematic exploration, backtracking"
}

def load_task(filepath):
    """Load a task from JSON file."""
    with open(filepath, 'r') as f:
        return json.load(f)

def grid_to_array(grid):
    """Convert grid to numpy array."""
    return np.array(grid)

def analyze_object_cognition(task):
    """Detect if task requires object cognition skills."""
    signals = []
    
    for pair in task['train']:
        inp = grid_to_array(pair['input'])
        out = grid_to_array(pair['output'])
        
        # Check for connected component operations
        inp_unique = len(np.unique(inp))
        out_unique = len(np.unique(out))
        
        # Object segmentation likely if colors are preserved or reduced
        if inp_unique >= out_unique:
            signals.append("object_segmentation")
        
        # Check if output is smaller (object extraction)
        if out.shape[0] < inp.shape[0] or out.shape[1] < inp.shape[1]:
            signals.append("object_extraction")
            
    return len(signals) > 0, signals

def analyze_numerosity(task):
    """Detect if task requires counting or numerical reasoning."""
    signals = []
    
    for pair in task['train']:
        inp = grid_to_array(pair['input'])
        out = grid_to_array(pair['output'])
        
        # Count non-background pixels of each color
        inp_counts = Counter(inp.flatten())
        out_counts = Counter(out.flatten())
        
        # Check if output size relates to input counts
        if out.shape[0] in inp_counts.values() or out.shape[1] in inp_counts.values():
            signals.append("count_to_size")
        
        # Check for repetition based on count
        for color in inp_counts:
            if color == 0:  # Skip background
                continue
            count = inp_counts[color]
            # Look for patterns repeated 'count' times in output
            if out_counts.get(color, 0) > inp_counts.get(color, 0):
                signals.append("count_based_repetition")
                break
    
    return len(signals) > 0, signals

def analyze_geometry(task):
    """Detect if task requires geometric reasoning."""
    signals = []
    
    for pair in task['train']:
        inp = grid_to_array(pair['input'])
        out = grid_to_array(pair['output'])
        
        # Check for rotation (shape changes but not content)
        if inp.shape != out.shape:
            if inp.shape[0] == out.shape[1] and inp.shape[1] == out.shape[0]:
                signals.append("rotation_90")
        
        # Check for reflection
        if np.array_equal(inp, np.flip(out, axis=0)):
            signals.append("reflect_vertical")
        if np.array_equal(inp, np.flip(out, axis=1)):
            signals.append("reflect_horizontal")
        
        # Check for symmetry in output
        if np.array_equal(out, np.flip(out, axis=0)) or np.array_equal(out, np.flip(out, axis=1)):
            signals.append("symmetry_creation")
            
    return len(signals) > 0, signals

def analyze_topology(task):
    """Detect if task requires topological reasoning."""
    signals = []
    
    for pair in task['train']:
        inp = grid_to_array(pair['input'])
        out = grid_to_array(pair['output'])
        
        # Check for border/boundary operations
        # If output emphasizes edges or borders
        inp_border = np.concatenate([
            inp[0, :], inp[-1, :], inp[:, 0], inp[:, -1]
        ])
        out_border = np.concatenate([
            out[0, :], out[-1, :], out[:, 0], out[:, -1]
        ])
        
        # If output border is different from input border
        if not np.array_equal(inp_border, out_border):
            signals.append("boundary_modification")
        
        # Check for fill operations (interior vs exterior)
        if inp.shape == out.shape:
            # Count color changes - fill might reduce them
            inp_changes = np.sum(inp[:-1,:] != inp[1:,:]) + np.sum(inp[:,:-1] != inp[:,1:])
            out_changes = np.sum(out[:-1,:] != out[1:,:]) + np.sum(out[:,:-1] != out[:,1:])
            
            if out_changes < inp_changes * 0.5:  # Significant reduction
                signals.append("region_fill")
    
    return len(signals) > 0, signals

def analyze_pattern_recognition(task):
    """Detect if task requires pattern recognition."""
    signals = []
    
    for pair in task['train']:
        inp = grid_to_array(pair['input'])
        out = grid_to_array(pair['output'])
        
        # Check for repetition in output
        if out.shape[0] > inp.shape[0] or out.shape[1] > inp.shape[1]:
            # Output is larger - might be tiling/repetition
            signals.append("pattern_tiling")
        
        # Check for periodic patterns
        # Simple check: is output a tiled version of input?
        if out.shape[0] % inp.shape[0] == 0 and out.shape[1] % inp.shape[1] == 0:
            tiles_v = out.shape[0] // inp.shape[0]
            tiles_h = out.shape[1] // inp.shape[1]
            if tiles_v > 1 or tiles_h > 1:
                signals.append("exact_tiling")
    
    return len(signals) > 0, signals

def analyze_transformation(task):
    """Detect if task requires transformation operations."""
    signals = []
    
    for pair in task['train']:
        inp = grid_to_array(pair['input'])
        out = grid_to_array(pair['output'])
        
        # Check for color mapping
        if inp.shape == out.shape:
            # Same shape - might be color transformation
            if not np.array_equal(inp, out):
                # Check if it's a simple color swap
                inp_colors = set(np.unique(inp))
                out_colors = set(np.unique(out))
                
                if inp_colors == out_colors:
                    signals.append("color_permutation")
                else:
                    signals.append("color_mapping")
        
        # Check for translation (content shifted)
        if inp.shape == out.shape:
            # Simple test: is there a shift where most pixels match?
            best_match = 0
            for dy in range(-2, 3):
                for dx in range(-2, 3):
                    if dy == 0 and dx == 0:
                        continue
                    # Calculate overlap region
                    y1_in, y2_in = max(0, dy), min(inp.shape[0], inp.shape[0] + dy)
                    x1_in, x2_in = max(0, dx), min(inp.shape[1], inp.shape[1] + dx)
                    y1_out, y2_out = max(0, -dy), min(out.shape[0], out.shape[0] - dy)
                    x1_out, x2_out = max(0, -dx), min(out.shape[1], out.shape[1] - dx)
                    
                    if y1_in < y2_in and x1_in < x2_in:
                        overlap = np.sum(inp[y1_in:y2_in, x1_in:x2_in] == 
                                        out[y1_out:y2_out, x1_out:x2_out])
                        best_match = max(best_match, overlap)
            
            total_pixels = inp.shape[0] * inp.shape[1]
            if best_match > total_pixels * 0.7:
                signals.append("translation")
    
    return len(signals) > 0, signals

def analyze_task(task, task_name=""):
    """Analyze a single task and identify required skills."""
    
    skill_analysis = {}
    
    # Analyze each skill category
    detected, signals = analyze_object_cognition(task)
    if detected:
        skill_analysis["object_cognition"] = signals
    
    detected, signals = analyze_numerosity(task)
    if detected:
        skill_analysis["numerosity"] = signals
    
    detected, signals = analyze_geometry(task)
    if detected:
        skill_analysis["geometry"] = signals
    
    detected, signals = analyze_topology(task)
    if detected:
        skill_analysis["topology"] = signals
    
    detected, signals = analyze_pattern_recognition(task)
    if detected:
        skill_analysis["pattern_recognition"] = signals
    
    detected, signals = analyze_transformation(task)
    if detected:
        skill_analysis["transformation"] = signals
    
    # All tasks require these meta-skills to some degree
    skill_analysis["hypothesis_testing"] = ["required_for_all"]
    skill_analysis["attention"] = ["required_for_all"]
    
    return skill_analysis

def analyze_dataset(data_dir="data/training", max_tasks=20):
    """Analyze multiple tasks and produce statistics."""
    
    task_files = sorted(Path(data_dir).glob("*.json"))[:max_tasks]
    
    all_analyses = {}
    skill_frequency = Counter()
    
    print(f"\n{'='*70}")
    print(f"ANALYZING ARC-AGI TASKS")
    print(f"{'='*70}\n")
    
    for task_file in task_files:
        task_name = task_file.stem
        task = load_task(task_file)
        
        analysis = analyze_task(task, task_name)
        all_analyses[task_name] = analysis
        
        # Count skills
        for skill in analysis:
            skill_frequency[skill] += 1
        
        # Print analysis
        print(f"üìã Task: {task_name}")
        print(f"   Train examples: {len(task['train'])}")
        print(f"   Test examples: {len(task['test'])}")
        print(f"   Detected skills:")
        
        for skill, signals in analysis.items():
            if skill in ["hypothesis_testing", "attention"]:
                continue  # Skip meta-skills in detailed view
            skill_desc = SKILLS.get(skill, "Unknown")
            print(f"      ‚Ä¢ {skill}: {', '.join(signals)}")
        
        print()
    
    # Print summary statistics
    print(f"\n{'='*70}")
    print(f"SKILL FREQUENCY SUMMARY ({max_tasks} tasks analyzed)")
    print(f"{'='*70}\n")
    
    for skill, count in skill_frequency.most_common():
        if skill in ["hypothesis_testing", "attention"]:
            continue
        percentage = (count / len(all_analyses)) * 100
        skill_desc = SKILLS[skill]
        print(f"{skill:20s} ({percentage:5.1f}%): {skill_desc}")
    
    return all_analyses, skill_frequency

if __name__ == "__main__":
    print("Waiting for dataset download to complete...")
    
    # Check if data exists
    if not os.path.exists("data/training"):
        print("‚ùå Training data not found. Please run download_dataset.py first.")
        exit(1)
    
    # Analyze a sample of tasks
    analyses, frequencies = analyze_dataset("data/training", max_tasks=20)
    
    print(f"\n‚úÖ Analysis complete! Examined {len(analyses)} tasks.")
    print(f"\nThese results validate our curriculum design.")
    print(f"All identified skills are covered in CURRICULUM.md.")
